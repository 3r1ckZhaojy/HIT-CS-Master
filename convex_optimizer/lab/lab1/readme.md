## 实验一

### 实验要求

熟悉基本的平台，搭建基本的优化算法实现和验证框架，对**无约束优化**问题进行求解和验证，尤其对**一阶、二阶和无导数**求解算法进行
实验。（可以整理作业代码）


### 实验内容

![img.png](img.png)

方便起见，可以就着作业2的第7题，来实验一下。


- [x] 无导数（这个可以直接使用Nelder-Mead）
- [x] 一阶(FR和GD)
- [x] 二阶(Newton，DFP,BFGS)

### 理论介绍

这里是一些关于无导数、一阶和二阶优化方法的介绍：

#### 无导数优化方法：

无导数优化方法是一种不需要计算目标函数梯度的优化方法。
这类方法通常在目标函数不可微或梯度计算困难的情况下使用。
一个典型的无导数优化方法是Nelder-Mead方法。
Nelder-Mead方法是一种基于单纯形搜索的方法，通过在搜索空间中移动单纯形来寻找最优解。
这种方法适用于低维问题，但在高维问题中可能效率较低。

#### 一阶优化方法：
一阶优化方法是基于目标函数的一阶导数（梯度）进行优化的方法。
这类方法通常在目标函数可微且梯度计算相对容易的情况下使用。以下是两种常见的一阶优化方法：

a. FR（Fletcher-Reeves）：Fletcher-Reeves是共轭梯度法的一种，它使用目标函数的梯度信息来寻找最优解。在每次迭代中，搜索方向是当前梯度与前一步搜索方向的线性组合。
这种方法通常比梯度下降法更快地收敛到最优解。

b. GD（梯度下降）：梯度下降是一种最常见的一阶优化方法，通过沿着目标函数梯度的负方向进行搜索来寻找最优解。梯度下降法的收敛速度可能较慢，但它适用于各种问题，包括高维问题。

#### 二阶优化方法：

二阶优化方法是基于目标函数的二阶导数（Hessian矩阵）进行优化的方法。
这类方法通常在目标函数具有良好的二阶连续性且Hessian矩阵计算相对容易的情况下使用。以下是三种常见的二阶优化方法：

a. Newton（牛顿法）：牛顿法是一种基于二阶导数信息的优化方法。
在每次迭代中，搜索方向是通过求解目标函数的Hessian矩阵与梯度的线性方程来确定的。
牛顿法通常具有较快的收敛速度，但在高维问题中可能受到Hessian矩阵计算和存储的限制。

b. DFP（Davidon-Fletcher-Powell）：DFP是一种拟牛顿法，它通过使用一阶导数信息来近似二阶导数信息。
DFP方法在每次迭代中更新一个近似Hessian矩阵，从而避免了直接计算Hessian矩阵的开销。
这种方法在许多问题中表现良好，尤其是在Hessian矩阵计算困难的情况下。

c. BFGS（Broyden-Fletcher-Goldfarb-Shanno）：BFGS是另一种拟牛顿法，它也使用一阶导数信息来近似二阶导数信息。
与DFP方法类似，BFGS方法在每次迭代中更新一个近似Hessian矩阵。
BFGS方法通常比DFP方法更稳定且收敛速度更快，因此在实践中更为常用。
这些优化方法在不同的问题和场景下具有各自的优势和局限性。
在实际应用中，选择合适的优化方法需要根据问题的特点和计算资源进行权衡。

### 实验结果
| 优化方法 | 迭代次数 | 最优解         |        最优值         |
| :-----: |:----:|:------------|:------------------:|
| 无导数 |  76  | [-1.   1.5] |       -1.25        |
| 一阶FR |  2   | [-1.   1.5] | -1.25 |
| 一阶GD |   1844   | [-1.   1.5] | -1.25|
| 二阶Newton |  2   | [-1.   1.5] | -1.25 |
| 二阶DFP |  2   | [-1.   1.5] | -1.25 |
| 二阶BFGS |  2   | [-1.   1.5] | -1.25|


